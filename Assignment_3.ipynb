{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jenil02/CS6910-Assignment3/blob/main/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B1RUJSBFG54",
        "outputId": "62fbb58e-8ba5-4a2e-9545-eeb79cf3f429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.23.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ],
      "source": [
        "pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14lSUj1gGDUG",
        "outputId": "ac3d0d5f-6799-4dd3-e6fd-594b972487e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjenilsheth\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5QDNMr5sFPpe"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.letter2index = {}\n",
        "        self.letter2count = {}\n",
        "        self.index2letter = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_letters = 2  # Count SOS and EOS\n",
        "\n",
        "    def addletter(self, letter): # making a dictionary of letters and their counts\n",
        "        if letter not in self.letter2index:\n",
        "            self.letter2index[letter] = self.n_letters\n",
        "            self.letter2count[letter] = 1\n",
        "            self.index2letter[self.n_letters] = letter\n",
        "            self.n_letters += 1\n",
        "        else:\n",
        "            self.letter2count[letter] += 1\n",
        "\n",
        "    def addword(self, letter): # adding a word to the dictionary\n",
        "        for letter in letter:\n",
        "            self.addletter(letter)\n",
        "\n",
        "    def decode(self, target):\n",
        "        return ' '.join([self.index2letter[i.get] for i in target])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kjdKRrTBFdkS"
      },
      "outputs": [],
      "source": [
        "def readLang(lang1, lang2, reverse=False): # read the file and make a dictionary of words of both languages\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    train_lines = open('/content/drive/MyDrive/hin_train.csv', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    val_lines = open('/content/drive/MyDrive/hin_valid.csv', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    train_pairs = [l.split(',') for l in train_lines]\n",
        "    val_pairs = [l.split(',') for l in val_lines]\n",
        "\n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "\n",
        "    for pair in train_pairs:\n",
        "        input_lang.addword(pair[0])\n",
        "        output_lang.addword(pair[1])\n",
        "    \n",
        "    for pair in val_pairs:\n",
        "        input_lang.addword(pair[0])\n",
        "        output_lang.addword(pair[1])\n",
        "\n",
        "    return train_pairs, val_pairs, input_lang, output_lang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5TSCkBNcFh_4"
      },
      "outputs": [],
      "source": [
        "def indexesFromword(lang, word): # convert a word to a list of indexes\n",
        "    return [lang.letter2index[letter] for letter in word]\n",
        "\n",
        "\n",
        "def tensorFromword(lang, word): # convert a word to a tensor\n",
        "    indexes = indexesFromword(lang, word)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair, input_lang, output_lang): # convert a pair of words to a pair of tensors\n",
        "    input_tensor = tensorFromword(input_lang, pair[0])\n",
        "    target_tensor = tensorFromword(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NPhDUOwcFqRg"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module): # encoder class\n",
        "    def __init__(self, type, input_size, emb_size, hidden_size, p, num_layers):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, emb_size)\n",
        "        self.rnn = nn.RNN(emb_size, hidden_size, num_layers, dropout = p)\n",
        "        self.gru = nn.GRU(emb_size, hidden_size, num_layers, dropout = p)\n",
        "        self.lstm = nn.LSTM(emb_size, hidden_size, num_layers, dropout = p)\n",
        "        self.type_t = type\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.dropout(self.embedding(input)).view(1, 1, -1) #embedding of word\n",
        "        output = embedded\n",
        "        \n",
        "        # giving output according to model type\n",
        "        if self.type_t == 'RNN':\n",
        "            output, hidden = self.rnn(output, hidden)\n",
        "        elif self.type_t == 'GRU':\n",
        "            output, hidden = self.gru(output, hidden)\n",
        "        elif self.type_t == 'LSTM':\n",
        "            output, hidden = self.lstm(output, hidden)\n",
        "\n",
        "        return output, hidden\n",
        "    \n",
        "    def initHidden(self): # initializing hidden layer\n",
        "        if self.type_t == 'LSTM':\n",
        "            return (torch.zeros(self.num_layers, 1, self.hidden_size, device=device), torch.zeros(self.num_layers, 1, self.hidden_size, device=device))\n",
        "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PKfIkkduFs-f"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module): # decoder class\n",
        "    def __init__(self, type, output_size, embedding_size, hidden_size, p, num_layers):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.type_t = type\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.dropout(self.embedding(input)).view(1, 1, -1) # embedding of word\n",
        "        output = F.relu(output) # applying activation to the input\n",
        "        \n",
        "        # giving output according to model type\n",
        "        if self.type_t == 'RNN':\n",
        "            output, hidden = self.rnn(output, hidden)\n",
        "        elif self.type_t == 'GRU':\n",
        "            output, hidden = self.gru(output, hidden)\n",
        "        elif self.type_t == 'LSTM':\n",
        "            output, hidden = self.lstm(output, hidden)\n",
        "\n",
        "        # softmax to get probabilities\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "    \n",
        "    def initHidden(self): # initializing hidden layer\n",
        "        if self.type_t == 'LSTM':\n",
        "            return (torch.zeros(self.num_layers, 1, self.hidden_size, device=device), torch.zeros(self.num_layers, 1, self.hidden_size, device=device))\n",
        "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TAGayBRbFwnQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RJ5J520LFz8K"
      },
      "outputs": [],
      "source": [
        "class Train(): # training class\n",
        "    def __init__(self, train_data, encoder, decoder, criterion, tfr = 0.5):\n",
        "        self.train_data = train_data\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.criterion = criterion\n",
        "        self.tfr = tfr\n",
        "        self.train_pairs, self.val_pairs, self.input_lang, self.output_lang = readLang('eng', 'hin')\n",
        "        self.training_pairs = [tensorsFromPair(self.train_pairs[i], self.input_lang, self.output_lang) for i in range(len(self.train_pairs))]\n",
        "\n",
        "    def train(self, input_tensor, target_tensor, encoder_optimizer, decoder_optimizer):\n",
        "        encoder_hidden = self.encoder.initHidden()\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "        encoder_outputs = torch.zeros(50, self.encoder.hidden_size, device=device)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        input_length = input_tensor.size(0)\n",
        "        target_length = target_tensor.size(0)\n",
        "\n",
        "        for i in range(input_length): # encoding a word\n",
        "            encoder_output, encoder_hidden = self.encoder(input_tensor[i], encoder_hidden)\n",
        "            # print(encoder_output.shape)\n",
        "            encoder_outputs[i] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "        decoder_hidden = encoder_hidden # encoder shares its hidden layer with decoder\n",
        "\n",
        "        use_teacher_forcing = True if random.random() < self.tfr else False\n",
        "\n",
        "        if use_teacher_forcing: \n",
        "            for i in range(target_length):\n",
        "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "                loss += self.criterion(decoder_output, target_tensor[i])\n",
        "                decoder_input = target_tensor[i] # teacher forcing\n",
        "\n",
        "        else:\n",
        "            for i in range(target_length):\n",
        "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "                topv, topi = decoder_output.topk(1) # top k predictions\n",
        "                decoder_input = topi.squeeze().detach() # detach from history as input\n",
        "                loss += self.criterion(decoder_output, target_tensor[i])\n",
        "                if decoder_input.item() == EOS_token: # if EOS token is predicted, stop\n",
        "                    break\n",
        "\n",
        "        loss.backward() \n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        return loss.item() / target_length\n",
        "\n",
        "\n",
        "    def trainIters(self, optimizer, learning_rate, n_iters = 69, print_every = 69, epochs=-1):\n",
        "        start = time.time()\n",
        "        print_loss_total = 0\n",
        "\n",
        "        if optimizer == 'SGD':\n",
        "            encoder_optimizer = optim.SGD(self.encoder.parameters(), lr = learning_rate)\n",
        "            decoder_optimizer = optim.SGD(self.decoder.parameters(), lr = learning_rate)\n",
        "        elif optimizer == 'Adam':\n",
        "            encoder_optimizer = optim.Adam(self.encoder.parameters(), lr = learning_rate)\n",
        "            decoder_optimizer = optim.Adam(self.decoder.parameters(), lr = learning_rate)\n",
        "        elif optimizer == 'RMSprop':\n",
        "            encoder_optimizer = optim.RMSprop(self.encoder.parameters(), lr = learning_rate)\n",
        "            decoder_optimizer = optim.RMSprop(self.decoder.parameters(), lr = learning_rate)\n",
        "        elif optimizer == 'NAdam':\n",
        "            encoder_optimizer = optim.NAdam(self.encoder.parameters(), lr = learning_rate)\n",
        "            decoder_optimizer = optim.NAdam(self.decoder.parameters(), lr = learning_rate)\n",
        "\n",
        "        if epochs != -1:\n",
        "            n_iters = len(self.train_pairs)\n",
        "        else:\n",
        "            train_loss_total = 0\n",
        "            for iter in tqdm(range(1, n_iters+1)):\n",
        "                training_pair = self.training_pairs[iter - 1]\n",
        "                input_tensor = training_pair[0]\n",
        "                target_tensor = training_pair[1]\n",
        "                loss = self.train(input_tensor, target_tensor, encoder_optimizer, decoder_optimizer)\n",
        "                train_loss_total += loss\n",
        "\n",
        "                if iter % print_every == 0:\n",
        "                    print_loss_avg = print_loss_total / print_every\n",
        "                    print_loss_total = 0\n",
        "                    print('%s (%d %d%%) %.4f' % (timeSince(start, iter/n_iters), iter, iter/n_iters*100, print_loss_avg))\n",
        "            train_acc = self.evaluateData(self.train_pairs)\n",
        "            valid_acc = self.evaluateData(self.val_pairs)\n",
        "            return train_acc, valid_acc\n",
        "\n",
        "        train_losss = []\n",
        "        valid_accs = []\n",
        "        train_accs = []\n",
        "        for j in range(epochs):\n",
        "            train_loss_total = 0\n",
        "            for iter in tqdm(range(1, n_iters+1)):\n",
        "                training_pair = self.training_pairs[iter - 1]\n",
        "                input_tensor = training_pair[0]\n",
        "                target_tensor = training_pair[1]\n",
        "                loss = self.train(input_tensor, target_tensor, encoder_optimizer, decoder_optimizer)\n",
        "                train_loss_total += loss\n",
        "                print_loss_total += loss\n",
        "\n",
        "                if iter % print_every == 0:\n",
        "                    print_loss_avg = print_loss_total / print_every\n",
        "                    print_loss_total = 0\n",
        "                    print('%s (%d %d%%) %.4f' % (timeSince(start, iter/n_iters), iter, iter/n_iters*100, print_loss_avg))\n",
        "            train_acc = self.evaluateData(self.train_pairs)\n",
        "            valid_acc = self.evaluateData(self.val_pairs)\n",
        "            train_losss.append(train_loss_total / n_iters)\n",
        "            valid_accs.append(valid_acc)\n",
        "            train_accs.append(train_acc)\n",
        "            wandb.log({'train_loss': train_loss_total / n_iters, 'train_acc': train_acc, 'valid_acc': valid_acc})\n",
        "        return train_losss, train_accs, valid_accs\n",
        "                    \n",
        "\n",
        "    def evaluate(self, word):\n",
        "        with torch.no_grad():\n",
        "            input_tensor = tensorFromword(self.input_lang, word)\n",
        "            input_length = input_tensor.size()[0]\n",
        "            encoder_hidden = self.encoder.initHidden()\n",
        "\n",
        "            encoder_outputs = torch.zeros(50, self.encoder.hidden_size, device=device)\n",
        "\n",
        "            for i in range(input_length): # encoding a word\n",
        "                encoder_output, encoder_hidden = self.encoder(input_tensor[i], encoder_hidden)\n",
        "                # print(encoder_output.shape)\n",
        "                encoder_outputs[i] += encoder_output[0, 0]\n",
        "\n",
        "            decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "            decoder_hidden = encoder_hidden # encoder shares its hidden layer with decoder\n",
        "\n",
        "            decoded_word = ''\n",
        "\n",
        "            for i in range(50):\n",
        "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "                topv, topi = decoder_output.topk(1) # top k predictions\n",
        "                if topi.item() == EOS_token:\n",
        "                    break\n",
        "                else:\n",
        "                    decoded_word += (self.output_lang.index2letter[topi.item()])\n",
        "                decoder_input = topi.squeeze().detach() # detach from history as input\n",
        "\n",
        "            return decoded_word\n",
        "        \n",
        "    def evaluateData(self, data):\n",
        "        acc = 0\n",
        "        for word,target in data:\n",
        "            acc += (self.evaluate(word) == target)\n",
        "        return acc / len(data)\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ImlwSQkhH9nU"
      },
      "outputs": [],
      "source": [
        "train_pairs, val_pairs, input_lang, output_lang = readLang('eng', 'hin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gWijFJXLF27-"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random', \n",
        "    'metric': {\n",
        "        'name': 'valid_acc',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "        'optimizer': {\n",
        "            'values': ['SGD', 'Adam', 'RMSprop', 'NAdam']\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-4, 5e-4, 0.001, 0.005, 0.01]\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [5, 10, 15, 20]\n",
        "        },\n",
        "        'hid_layers': {\n",
        "            'values': [1, 2, 3, 4]\n",
        "        },\n",
        "        'emb_size': {\n",
        "            'values': [64, 128, 256, 512]\n",
        "        },\n",
        "        'hidden_size': {\n",
        "            'values': [64, 128, 256, 512]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0, 0.1, 0.2, 0.3, 0.4]\n",
        "        },\n",
        "        'type_t': {\n",
        "            'values': ['RNN', 'LSTM', 'GRU']\n",
        "        }\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57hYVxrsF6KR"
      },
      "outputs": [],
      "source": [
        "def run():\n",
        "  config_defaults = {\n",
        "        'optimizer': 'Adam',\n",
        "        'learning_rate': 0.005,\n",
        "        'epochs': 10,\n",
        "        'hid_layers': 1,\n",
        "        'emb_size': 256,\n",
        "        'hidden_size': 256,\n",
        "        'dropout': 0.1,\n",
        "        'type_t': 'GRU'\n",
        "  }\n",
        "  wandb.init(config=config_defaults)\n",
        "  config = wandb.config\n",
        "  encoder = Encoder(config.type_t, input_lang.n_letters, config.emb_size, config.hidden_size, config.dropout, config.hid_layers).to(device)\n",
        "  decoder = Decoder(config.type_t, output_lang.n_letters, config.emb_size, config.hidden_size, config.dropout, config.hid_layers).to(device)\n",
        "  train = Train(train_pairs, encoder, decoder, nn.NLLLoss(), config.dropout)\n",
        "  train.trainIters(config.optimizer, config.learning_rate,print_every= 1000, epochs=config.epochs)\n",
        "\n",
        "  wandb.finish()\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project='assignment-3')\n",
        "wandb.agent(sweep_id, function=run, count=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PxouD3BKY4W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ivThNz5Qc4580aoiq33iR6mX3K63495q",
      "authorship_tag": "ABX9TyMHD0HTr+vzNuYrTbnhwMZ9",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}