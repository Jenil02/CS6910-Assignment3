{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B1RUJSBFG54",
        "outputId": "bc203b11-7a17-446e-dd55-746405ba08ba"
      },
      "outputs": [],
      "source": [
        "pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "14lSUj1gGDUG",
        "outputId": "36e2038c-1e8d-499d-e134-fc0779d4024a"
      },
      "outputs": [],
      "source": [
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QDNMr5sFPpe"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.letter2index = {}\n",
        "        self.letter2count = {}\n",
        "        self.index2letter = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_letters = 2  # Count SOS and EOS\n",
        "\n",
        "    def addletter(self, letter): # making a dictionary of letters and their counts\n",
        "        if letter not in self.letter2index:\n",
        "            self.letter2index[letter] = self.n_letters\n",
        "            self.letter2count[letter] = 1\n",
        "            self.index2letter[self.n_letters] = letter\n",
        "            self.n_letters += 1\n",
        "        else:\n",
        "            self.letter2count[letter] += 1\n",
        "\n",
        "    def addword(self, letter): # adding a word to the dictionary\n",
        "        for letter in letter:\n",
        "            self.addletter(letter)\n",
        "\n",
        "    def decode(self, target):\n",
        "        return ' '.join([self.index2letter[i.get] for i in target])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjdKRrTBFdkS"
      },
      "outputs": [],
      "source": [
        "def readLang(lang1, lang2, reverse=False): # read the file and make a dictionary of words of both languages\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    train_lines = open('/content/drive/MyDrive/hin_train.csv', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    val_lines = open('/content/drive/MyDrive/hin_valid.csv', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    test_lines = open('/content/drive/MyDrive/hin_test.csv', encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    train_pairs = [l.split(',') for l in train_lines]\n",
        "    val_pairs = [l.split(',') for l in val_lines]\n",
        "    test_pairs = [l.split(',') for l in test_lines]\n",
        "\n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "\n",
        "    for pair in train_pairs:\n",
        "      input_lang.addword(pair[0])\n",
        "      output_lang.addword(pair[1])\n",
        "    \n",
        "    for pair in val_pairs:\n",
        "      input_lang.addword(pair[0])\n",
        "      output_lang.addword(pair[1])\n",
        "    for pair in test_pairs:\n",
        "      input_lang.addword(pair[0])\n",
        "      output_lang.addword(pair[1])\n",
        "\n",
        "    return train_pairs, val_pairs, test_pairs, input_lang, output_lang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TSCkBNcFh_4"
      },
      "outputs": [],
      "source": [
        "def indexesFromword(lang, word): # convert a word to a list of indexes\n",
        "    return [lang.letter2index[letter] for letter in word]\n",
        "\n",
        "\n",
        "def tensorFromword(lang, word): # convert a word to a tensor\n",
        "    indexes = indexesFromword(lang, word)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair, input_lang, output_lang): # convert a pair of words to a pair of tensors\n",
        "    input_tensor = tensorFromword(input_lang, pair[0])\n",
        "    target_tensor = tensorFromword(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPhDUOwcFqRg"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module): # encoder class\n",
        "    def __init__(self, type, input_size, emb_size, hidden_size, p, num_layers):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(input_size, emb_size)\n",
        "        self.rnn = nn.RNN(emb_size, hidden_size, num_layers, dropout = p)\n",
        "        self.gru = nn.GRU(emb_size, hidden_size, num_layers, dropout = p)\n",
        "        self.lstm = nn.LSTM(emb_size, hidden_size, num_layers, dropout = p)\n",
        "        self.type_t = type\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1) #embedding of word\n",
        "        embedded = self.dropout(embedded)\n",
        "        output = embedded\n",
        "        \n",
        "        # giving output according to model type\n",
        "        if self.type_t == 'RNN':\n",
        "            output, hidden = self.rnn(output, hidden)\n",
        "        elif self.type_t == 'GRU':\n",
        "            output, hidden = self.gru(output, hidden)\n",
        "        elif self.type_t == 'LSTM':\n",
        "            output, hidden = self.lstm(output, hidden)\n",
        "\n",
        "        return output, hidden\n",
        "    \n",
        "    def initHidden(self): # initializing hidden layer\n",
        "        if self.type_t == 'LSTM':\n",
        "            return (torch.zeros(self.num_layers, 1, self.hidden_size, device=device), torch.zeros(self.num_layers, 1, self.hidden_size, device=device))\n",
        "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKfIkkduFs-f"
      },
      "outputs": [],
      "source": [
        "class AttnDecoder(nn.Module): # decoder class\n",
        "    def __init__(self, type, output_size, hidden_size, p, num_layers):\n",
        "        super(AttnDecoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attn = nn.Linear(hidden_size*2, 50)\n",
        "        self.attn_combine = nn.Linear(hidden_size*2, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, dropout = p)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, dropout = p)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, dropout = p)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.type_t = type\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1) # embedding of word\n",
        "        embedded = self.dropout(embedded)\n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1) # attention weights\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0)) # attention applied\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "        output = F.relu(output)\n",
        "\n",
        "        # giving output according to model type\n",
        "        if self.type_t == 'RNN':\n",
        "            output, hidden = self.rnn(output, hidden)\n",
        "        elif self.type_t == 'GRU':\n",
        "            output, hidden = self.gru(output, hidden)\n",
        "        elif self.type_t == 'LSTM':\n",
        "            output, hidden = self.lstm(output, hidden)\n",
        "\n",
        "        # softmax to get probabilities\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "    \n",
        "    def initHidden(self): # initializing hidden layer\n",
        "        if self.type_t == 'LSTM':\n",
        "            return (torch.zeros(self.num_layers, 1, self.hidden_size, device=device), torch.zeros(self.num_layers, 1, self.hidden_size, device=device))\n",
        "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAGayBRbFwnQ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ5J520LFz8K"
      },
      "outputs": [],
      "source": [
        "class Train(): # training class\n",
        "    def __init__(self, train_data, encoder, decoder, criterion, tfr = 0.5):\n",
        "        self.train_data = train_data\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.criterion = criterion\n",
        "        self.tfr = tfr\n",
        "        self.train_pairs, self.val_pairs, self.test_pairs, self.input_lang, self.output_lang = readLang('eng', 'hin')\n",
        "        self.training_pairs = [tensorsFromPair(self.train_pairs[i], self.input_lang, self.output_lang) for i in range(len(self.train_pairs))]\n",
        "\n",
        "    def train(self, input_tensor, target_tensor, encoder_optimizer, decoder_optimizer):\n",
        "        encoder_hidden = self.encoder.initHidden()\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "        encoder_outputs = torch.zeros(50, self.encoder.hidden_size, device=device)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        input_length = input_tensor.size(0)\n",
        "        target_length = target_tensor.size(0)\n",
        "\n",
        "        for i in range(input_length): # encoding a word\n",
        "            encoder_output, encoder_hidden = self.encoder(input_tensor[i], encoder_hidden)\n",
        "            # print(encoder_output.shape)\n",
        "            encoder_outputs[i] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "        decoder_hidden = encoder_hidden # encoder shares its hidden layer with decoder\n",
        "\n",
        "        use_teacher_forcing = True if random.random() < self.tfr else False\n",
        "        \n",
        "        if use_teacher_forcing: \n",
        "            for i in range(target_length):\n",
        "                decoder_output, decoder_hidden, decoder_attention = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "                loss += self.criterion(decoder_output, target_tensor[i])\n",
        "                decoder_input = target_tensor[i] # teacher forcing\n",
        "\n",
        "        else:\n",
        "            for i in range(target_length):\n",
        "                decoder_output, decoder_hidden, decoder_attention = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "                topv, topi = decoder_output.topk(1) # top k predictions\n",
        "                decoder_input = topi.squeeze().detach() # detach from history as input\n",
        "                loss += self.criterion(decoder_output, target_tensor[i])\n",
        "                if decoder_input.item() == EOS_token: # if EOS token is predicted, stop\n",
        "                    break\n",
        "\n",
        "        loss.backward() \n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        return loss.item() / target_length\n",
        "\n",
        "\n",
        "    def trainIters(self, optimizer, learning_rate, n_iters = 69, print_every = 69, epochs=-1):\n",
        "        start = time.time()\n",
        "        print_loss_total = 0\n",
        "\n",
        "        if optimizer == 'SGD':\n",
        "            encoder_optimizer = optim.SGD(self.encoder.parameters(), lr = learning_rate)\n",
        "            decoder_optimizer = optim.SGD(self.decoder.parameters(), lr = learning_rate)\n",
        "        elif optimizer == 'Adam':\n",
        "            encoder_optimizer = optim.Adam(self.encoder.parameters(), lr = learning_rate)\n",
        "            decoder_optimizer = optim.Adam(self.decoder.parameters(), lr = learning_rate)\n",
        "        elif optimizer == 'RMSprop':\n",
        "            encoder_optimizer = optim.RMSprop(self.encoder.parameters(), lr = learning_rate)\n",
        "            decoder_optimizer = optim.RMSprop(self.decoder.parameters(), lr = learning_rate)\n",
        "        elif optimizer == 'NAdam':\n",
        "            encoder_optimizer = optim.NAdam(self.encoder.parameters(), lr = learning_rate)\n",
        "            decoder_optimizer = optim.NAdam(self.decoder.parameters(), lr = learning_rate)\n",
        "\n",
        "        if epochs != -1: # if epochs are specified\n",
        "            n_iters = len(self.train_pairs)\n",
        "        else:\n",
        "            train_loss_total = 0\n",
        "            for iter in tqdm(range(1, n_iters+1)):\n",
        "                training_pair = self.training_pairs[iter - 1]\n",
        "                input_tensor = training_pair[0]\n",
        "                target_tensor = training_pair[1]\n",
        "                loss = self.train(input_tensor, target_tensor, encoder_optimizer, decoder_optimizer)\n",
        "                train_loss_total += loss\n",
        "\n",
        "                if iter % print_every == 0:\n",
        "                    print_loss_avg = print_loss_total / print_every\n",
        "                    print_loss_total = 0\n",
        "                    print('%s (%d %d%%) %.4f' % (timeSince(start, iter/n_iters), iter, iter/n_iters*100, print_loss_avg))\n",
        "            train_acc = self.evaluateData(self.train_pairs) #evaluating the model on train pairs\n",
        "            valid_acc = self.evaluateData(self.val_pairs) # evaluating the model on validation pairs\n",
        "            return train_acc, valid_acc\n",
        "\n",
        "        train_losss = []\n",
        "        valid_accs = []\n",
        "        train_accs = []\n",
        "        for j in range(epochs):\n",
        "            train_loss_total = 0\n",
        "            for iter in tqdm(range(1, n_iters+1)):\n",
        "                training_pair = self.training_pairs[iter - 1]\n",
        "                input_tensor = training_pair[0]\n",
        "                target_tensor = training_pair[1]\n",
        "                loss = self.train(input_tensor, target_tensor, encoder_optimizer, decoder_optimizer)\n",
        "                train_loss_total += loss\n",
        "                print_loss_total += loss\n",
        "\n",
        "                if iter % print_every == 0:\n",
        "                    print_loss_avg = print_loss_total / print_every\n",
        "                    print_loss_total = 0\n",
        "                    print('%s (%d %d%%) %.4f' % (timeSince(start, iter/n_iters), iter, iter/n_iters*100, print_loss_avg))\n",
        "            train_acc = self.evaluateData(self.train_pairs)\n",
        "            valid_acc = self.evaluateData(self.val_pairs)\n",
        "            train_losss.append(train_loss_total / n_iters)\n",
        "            valid_accs.append(valid_acc)\n",
        "            train_accs.append(train_acc)\n",
        "            print({'train_loss': train_loss_total / n_iters, 'train_acc': train_acc, 'valid_acc': valid_acc})\n",
        "            wandb.log({'train_loss': train_loss_total / n_iters, 'train_acc': train_acc, 'valid_acc': valid_acc})\n",
        "        return train_losss, train_accs, valid_accs\n",
        "                    \n",
        "\n",
        "    def evaluate(self, word):\n",
        "        with torch.no_grad():\n",
        "            input_tensor = tensorFromword(self.input_lang, word)\n",
        "            input_length = input_tensor.size()[0]\n",
        "            encoder_hidden = self.encoder.initHidden()\n",
        "\n",
        "            encoder_outputs = torch.zeros(50, self.encoder.hidden_size, device=device)\n",
        "\n",
        "            for i in range(input_length): # encoding a word\n",
        "                encoder_output, encoder_hidden = self.encoder(input_tensor[i], encoder_hidden)\n",
        "                # print(encoder_output.shape)\n",
        "                encoder_outputs[i] += encoder_output[0, 0]\n",
        "\n",
        "            decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "            decoder_hidden = encoder_hidden # encoder shares its hidden layer with decoder\n",
        "\n",
        "            decoded_word = ''\n",
        "            decoder_attentions = torch.zeros(50, 50)\n",
        "\n",
        "            for j in range(50):\n",
        "                decoder_output, decoder_hidden, decoder_attention = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "                decoder_attentions[j] = decoder_attention.data\n",
        "                topv, topi = decoder_output.topk(1) # top k predictions\n",
        "                if topi.item() == EOS_token:\n",
        "                    break\n",
        "                else:\n",
        "                    decoded_word += (self.output_lang.index2letter[topi.item()])\n",
        "                decoder_input = topi.squeeze().detach() # detach from history as input\n",
        "\n",
        "            return decoded_word, decoder_attentions[:j+1]\n",
        "        \n",
        "    def evaluateData(self, data):\n",
        "        acc = 0\n",
        "        for word,target in data:\n",
        "            output_word, attentions = self.evaluate(word)\n",
        "            acc += (output_word == target)\n",
        "        return acc / len(data)\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImlwSQkhH9nU"
      },
      "outputs": [],
      "source": [
        "train_pairs, val_pairs, test_pairs, input_lang, output_lang = readLang('eng', 'hin')\n",
        "\n",
        "# encoder = Encoder('GRU', input_lang.n_letters, 512, 512, 0, 1).to(device)\n",
        "# decoder = AttnDecoder('GRU', output_lang.n_letters, 512, 0, 1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlOnu_3lUpH-",
        "outputId": "ce0b70c6-53b6-4e2b-c7a6-64cd4add1094"
      },
      "outputs": [],
      "source": [
        "# train = Train(train_pairs, encoder, decoder, nn.NLLLoss())\n",
        "# train.trainIters('SGD', 0.01, print_every=1000, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWijFJXLF27-"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'random', \n",
        "    'metric': {\n",
        "        'name': 'valid_acc',\n",
        "        'goal': 'maximize' # goal is to maximize the validation accuracy\n",
        "    },\n",
        "    'parameters': {\n",
        "        'optimizer': {\n",
        "            'values': ['SGD', 'Adam', 'RMSprop']\n",
        "        },\n",
        "        'learning_rate': {\n",
        "            'values': [1e-4, 5e-4, 0.001, 0.005]\n",
        "        },\n",
        "        'epochs': {\n",
        "            'values': [10]\n",
        "        },\n",
        "        'hid_layers': {\n",
        "            'values': [1]\n",
        "        },\n",
        "        'emb_size': {\n",
        "            'values': [64, 128, 256, 512]\n",
        "        },\n",
        "        'hidden_size': {\n",
        "            'values': [64, 128, 256, 512]\n",
        "        },\n",
        "        'dropout': {\n",
        "            'values': [0, 0.1, 0.2, 0.3]\n",
        "        },\n",
        "        'type_t': {\n",
        "            'values': ['RNN', 'LSTM', 'GRU']\n",
        "        }\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57hYVxrsF6KR"
      },
      "outputs": [],
      "source": [
        "def run():\n",
        "    # Default values for hyper-parameters\n",
        "    config_defaults = {\n",
        "        'optimizer': 'Adam',\n",
        "        'learning_rate': 0.005,\n",
        "        'epochs': 10,\n",
        "        'hid_layers': 1,\n",
        "        'emb_size': 256,\n",
        "        'hidden_size': 256,\n",
        "        'dropout': 0.1,\n",
        "        'type_t': 'GRU'\n",
        "    }\n",
        "    wandb.init(config=config_defaults) # Initialize a new wandb run\n",
        "    config = wandb.config # config saves hyperparameters and inputs\n",
        "    encoder = Encoder(config.type_t, input_lang.n_letters, config.emb_size, config.hidden_size, config.dropout, config.hid_layers).to(device)\n",
        "    decoder = AttnDecoder(config.type_t, output_lang.n_letters, config.hidden_size, config.dropout, config.hid_layers).to(device)\n",
        "    train = Train(train_pairs, encoder, decoder, nn.NLLLoss())\n",
        "    train.trainIters(config.optimizer, config.learning_rate,print_every= 1000, epochs=config.epochs)\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project='assignment-3-attention')\n",
        "wandb.agent(sweep_id, function=run, count=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PxouD3BKY4W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO6mro8afuzmdA9/cyeHe29",
      "gpuType": "T4",
      "mount_file_id": "1ivThNz5Qc4580aoiq33iR6mX3K63495q",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
